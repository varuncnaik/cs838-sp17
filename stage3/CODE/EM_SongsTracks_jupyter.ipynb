{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matching Songs and Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This IPython notebook explains the workflow of matching two tables using *py_entitymatching*. Our goal is to come up with a workflow to match songs and tracks from Songs.csv and Tracks.csv. Specifically, we want to maximize F1. The datasets contain information about the songs, artists and year they were performed.\n",
    "\n",
    "First, we need to import *py_entitymatching* package and other libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the versions\n",
    "print('python version: ' + sys.version )\n",
    "print('pandas version: ' + pd.__version__ )\n",
    "print('magellan version: ' + em.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the input tables\n",
    "\n",
    "We begin by loading the input tables A and B from songs.csv and tracks.csv respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the path of the ipython notebook as the current path\n",
    "os.getcwd()\n",
    "orig_dir = os.getcwd()\n",
    "os.chdir(orig_dir)\n",
    "\n",
    "# Get the paths\n",
    "path_A = '../DATA/songs.csv'\n",
    "path_B = '../DATA/tracks.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we read the input tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = em.read_csv_metadata(path_A, key='id')\n",
    "B = em.read_csv_metadata(path_B, key='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of tuples in A: ' + str(len(A)))\n",
    "print('Number of tuples in B: ' + str(len(B)))\n",
    "print('Number of tuples in A X B (i.e the cartesian product): ' + str(len(A)*len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Downsample the input tables\n",
    "As the input tables are large we downsample the input tables to obtain sample_A and sample_B from A and B respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_A, sample_B = em.down_sample(A, B, size=5000, y_param=1, show_progress=True)\n",
    "print('Number of tuples in sample_A: ' + str(len(sample_A))) \n",
    "print('Number of tuples in sample_B: ' + str(len(sample_B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Block tables to get candidate set\n",
    "\n",
    "Before we do the matching, we would like to remove the obviously non-matching tuple pairs from the input tables. This would reduce the number of tuple pairs considered for matching.\n",
    "\n",
    "For the matching problem at hand, we know that two songs with different titles or different artist names will not match. We identified all (song, track) pairs such that song.title shared at least one word with track.song, and song.artist_name shared at least one word with track.artists. We noticed that Magellanâ€™s overlap blocker only performs blocking on a single column, so for convenience, we initially performed black-box blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define functions for black-box blocking\n",
    "def match(ltup, rtup):\n",
    "    '''\n",
    "    Returns True if (ltup, rtup) should be dropped, or False if (ltup, rtup) is\n",
    "    a candidate. Called by bb_block().\n",
    "    '''\n",
    "    l_song_withstop = str(ltup['title']).lower().split()\n",
    "    r_song_withstop = str(rtup['song']).lower().split()\n",
    "    l_artist_withstop = str(ltup['artist_name']).lower().split()\n",
    "    r_artist_withstop = str(rtup['artists']).lower().replace('+', ' ').split()\n",
    "    stopwords = ('the', 'a')\n",
    "    l_song = set(w for w in l_song_withstop if w not in stopwords)\n",
    "    r_song = set(w for w in r_song_withstop if w not in stopwords)\n",
    "    l_artist = set(w for w in l_artist_withstop if w not in stopwords)\n",
    "    r_artist = set(w for w in r_artist_withstop if w not in stopwords)\n",
    "\n",
    "    # If no overlap among artists or no overlap among songs, then drop\n",
    "    return l_artist.isdisjoint(r_artist) or l_song.isdisjoint(r_song)\n",
    "\n",
    "def bb_block(sample_A, sample_B):\n",
    "    '''\n",
    "    Returns a DataFrame of candidate pairs by performing black-box blocking.\n",
    "    '''\n",
    "    sample_B['artists'] = sample_B['artists'].str.replace('+', ' + ')\n",
    "\n",
    "    # Create black box blocker\n",
    "    bb = em.BlackBoxBlocker()\n",
    "    bb.set_black_box_function(match)\n",
    "    bbC = bb.block_tables(sample_A, sample_B,\n",
    "                        l_output_attrs=['id', 'title', 'artist_name', 'year'],\n",
    "                        r_output_attrs=['id', 'title', 'year', 'episode', 'song', 'artists'],\n",
    "    )\n",
    "\n",
    "    return bbC\n",
    "\n",
    "sample_A = em.read_csv_metadata('../DATA/sample_A.csv', key='id')\n",
    "sample_B = em.read_csv_metadata('../DATA/sample_B.csv', key='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Apply the blocker on the downsampled tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tuple pairs considered for matching is reduced to 5223 (from 20900000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = bb_block(sample_A, sample_B)\n",
    "len(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Debugging the blocker output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would want to make sure that the blocker did not drop any potential matches. We could debug the blocker output in *py_entitymatching* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Debug blocker output\n",
    "dbg = em.debug_blocker(C, sample_A, sample_B, attr_corres=[('title', 'song'),('artist_name', 'artists')], output_size=20)\n",
    "dbg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Apply the modified blocker on the downsampled tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above blackbox approach was prohibitively slow, taking roughly 20 minutes to run on the downsampled tables. Clearly, black-box blocking would not scale to the entirety of the Song and Track tables during the production stage. Hence we modified our blocking sequence to include the overlap blocker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display first few tuple pairs from the debug_blocker's output\n",
    "def overlap_block(sample_A, sample_B):\n",
    "    '''\n",
    "    Returns a DataFrame of candidate pairs by performing overlap blocking.\n",
    "    Performs overlap blocking on title, then overlap blocking on artist, then\n",
    "    black-box blocking to guarantee the same results as bb_block().\n",
    "    '''\n",
    "    # many artist names in sample_B are something like \n",
    "    # 'budda+wc+ice cube+mack 10+westside connection'\n",
    "    # change this to 'budda + wc + ice cube + mack 10 + westside connection'\n",
    "    sample_B['artists'] = sample_B['artists'].str.replace('+', ' + ')\n",
    "    \n",
    "    # Create overlap blocker\n",
    "    # If no overlap among artists or no overlap among songs, then drop\n",
    "    ob = em.OverlapBlocker()\n",
    "    ob.stop_words = ['a', 'the']\n",
    "    ob.regex_punctuation = re.compile(r'')\n",
    "    obC1 = ob.block_tables(sample_A, sample_B, 'title', 'song',\n",
    "                        l_output_attrs=['id', 'title', 'artist_name', 'year'],\n",
    "                        r_output_attrs=['id', 'title', 'year', 'episode', 'song', 'artists'],\n",
    "                        rem_stop_words=True,\n",
    "    )\n",
    "    ob.regex_punctuation = re.compile(r'\\+')\n",
    "    obC2 = ob.block_candset(obC1, 'artist_name', 'artists', rem_stop_words=True)\n",
    "\n",
    "    # Create black box blocker\n",
    "    # Necessary because overlap blocker behaves incorrectly when words contain Unicode characters\n",
    "    bb = em.BlackBoxBlocker()\n",
    "    bb.set_black_box_function(match)\n",
    "    bbC = bb.block_candset(obC2)\n",
    "    return bbC\n",
    "\n",
    "C = overlap_block(sample_A, sample_B)\n",
    "len(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We debugged the blocker output again to check if the current blocker sequence is dropping any potential matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display first few rows from the debugger output\n",
    "dbg = em.debug_blocker(C, sample_A, sample_B, attr_corres=[('title', 'song'),('artist_name', 'artists')], output_size=20)\n",
    "dbg.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the current blocker sequence does not drop obvious potential matches, and we can proceed with the matching step now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## d. Stop modifying the blocker\n",
    "We decide to stop blocking when we observed very less potential matches were dropped and we had developed a blocker sequence which was reasonably fast i.e, it took less than 30s to perform blocking on the downsampled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sampling and labeling the candidate set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we randomly sample 500 tuple pairs for labeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample candidate set\n",
    "S = em.sample_table(C, 500)\n",
    "S.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we label the sampled candidate set. Specifically we would enter 1 for a match and 0 for a non-match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Label S\n",
    "# G = em.label_table(S, label_column_name='gold_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook, we will load in a pre-labeled dataset (of 500 tuple pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = em.read_csv_metadata('../DATA/G.csv', key='_id')\n",
    "G.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Splitting the labeled data into development and evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we split the labeled data into two sets: development (I) and evaluation (J). Specifically, the development set is used to come up with the best learning-based matcher, and the evaluation set is used to evaluate the selected matcher on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split S into development set (I) and evaluation set (J)\n",
    "# G.head(10)\n",
    "# IJ = em.split_train_test(G, train_proportion=0.7, random_state=0)\n",
    "# I = IJ['train']\n",
    "# J = IJ['test']\n",
    "\n",
    "#for the purpose of this notebook read the already saved train and test files \n",
    "I = em.read_csv_metadata('../DATA/I.csv', key='_id', ltable=sample_A, rtable=sample_B, fk_ltable='ltable_id', fk_rtable='rtable_id')\n",
    "J = em.read_csv_metadata('../DATA/J.csv', key='_id', ltable=sample_A, rtable=sample_B, fk_ltable='ltable_id', fk_rtable='rtable_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating a set of learning-based matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The six learning-based matcher types in Magellan are Decision Tree, Random Forest, SVM, Naive Bayes, Logistic Regression, and Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name='NaiveBayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Selecting the best matcher using I. This step include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Creating features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a set of features for the development set. *py_entitymatching* provides a way to automatically generate features based on the attributes in the input tables. We automatically generate the features by providing the correspondence between the two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate features\n",
    "match_t = em.get_tokenizers_for_matching()\n",
    "match_s = em.get_sim_funs_for_matching()\n",
    "atypes1 = em.get_attr_types(sample_A)\n",
    "atypes2 = em.get_attr_types(sample_B)\n",
    "match_c = em.get_attr_corres(sample_A, sample_B)\n",
    "match_c['corres'] = [('title', 'song'),('artist_name', 'artists')]\n",
    "match_f = em.get_features(sample_A, sample_B, atypes1, atypes2, match_c, match_t, match_s)\n",
    "feature_table = match_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List the names of the features generated\n",
    "feature_table['feature_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Convert the I into a set of feature vectors using F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H = em.extract_feature_vecs(I, feature_table=feature_table, attrs_before= ['_id', 'ltable_id', 'rtable_id'], attrs_after='gold_labels', show_progress=False)\n",
    "# Display first few rows\n",
    "H.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### c. Filling in the missing values if any\n",
    "We do not have any missing values as the tuples with missing values are filtered out by the blocker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### d. Selecting the best matcher using cross-validation\n",
    "Now, we select the best matcher using k-fold cross-validation. We used ten fold cross validation and use 'F1' metric to select the best matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the best ML matcher using CV\n",
    "attrs_to_be_excluded = ['_id', 'ltable_id', 'rtable_id', 'gold_labels']\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=H, \n",
    "        exclude_attrs=attrs_to_be_excluded,\n",
    "        k=10,\n",
    "        target_attr='gold_labels', metric='f1', random_state=0)\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Debugging matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the best matcher is Random Forest. However it is still not maximizing F1. We debug the matcher to see what might be wrong.\n",
    "To do this, first we split the feature vectors into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Split feature vectors into train and test\n",
    "UV = em.split_train_test(H, train_proportion=0.5)\n",
    "U = UV['train']\n",
    "V = UV['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we debug the matcher using GUI. Since the best matcher we found was random forest matcher, we went ahead debugging it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debug decision tree using GUI\n",
    "em.vis_debug_rf(rf, U, V, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold_labels'],\n",
    "        target_attr='gold_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After debugging it was found that there were errors in labeling. To rectify this, we looked over the labeling to check for errors and fix them. In the dev set I, there were 5 errors found; all were instances where the tuple pairs were a match (should be 1) but were marked as not a match (mistakenly labeled as 0). After fixing the labeling the matcher was run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the best ML matcher using CV\n",
    "attrs_to_be_excluded = ['_id', 'ltable_id', 'rtable_id', 'gold_labels']\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=H, \n",
    "        exclude_attrs=attrs_to_be_excluded,\n",
    "        k=5,\n",
    "        target_attr='gold_labels', metric='f1', random_state=0)\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, observe the best matcher which is achieving a better F1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  8. Evaluating the matching output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the matching outputs for the evaluation set typically involves the following four steps:\n",
    "1. Converting the evaluation set to feature vectors\n",
    "2. Training matcher using the feature vectors extracted from the development set\n",
    "3. Predicting the evaluation set using the trained matcher\n",
    "4. Evaluating the predicted matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Converting the evaluation set to  feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we convert to the feature vectors (using the feature table and the evaluation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert J into a set of feature vectors using feature table\n",
    "L = em.extract_feature_vecs(J, feature_table=feature_table,\n",
    "                            attrs_after='gold_labels', show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Training the selected matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the matcher using all of the feature vectors from the development set. We have found random forest as the selected matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train using feature vectors from I \n",
    "rf.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold_labels'], \n",
    "       target_attr='gold_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Predicting the matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we predict the matches for the evaluation set (using the feature vectors extracted from it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on L \n",
    "predictions = rf.predict(table=L, exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold_labels'], \n",
    "              append=True, target_attr='predicted', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Computing the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the accuracy of predicted outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'gold_labels', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
